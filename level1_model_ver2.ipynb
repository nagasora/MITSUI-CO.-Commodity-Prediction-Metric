{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMlaSMTU+5Gc9Udy8TWdZmE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nagasora/MITSUI-CO.-Commodity-Prediction-Metric/blob/main/level1_model_ver2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GoRud6EBbakt",
        "outputId": "a756352e-1d8f-4304-b976-4444d60dc6cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "f = open(\"/content/drive/MyDrive/kaggle notebook/kaggle.json\")\n",
        "json_data = json.load(f)\n",
        "os.environ['KAGGLE_USERNAME'] = json_data['username']\n",
        "os.environ['KAGGLE_KEY'] = json_data['key']\n"
      ],
      "metadata": {
        "id": "KIaShcxscEsm"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#APIコマンドを入力\n",
        "!kaggle competitions download -c mitsui-commodity-prediction-challenge"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nkeknp1ucJQO",
        "outputId": "56cefe6c-5057-4070-a6bd-1028f97bcb8a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading mitsui-commodity-prediction-challenge.zip to /content\n",
            "\r  0% 0.00/9.94M [00:00<?, ?B/s]\n",
            "\r100% 9.94M/9.94M [00:00<00:00, 1.11GB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "\n",
        "# Specify the path to the zip file in Google Drive\n",
        "zip_file_path = \"/content/mitsui-commodity-prediction-challenge.zip\"\n",
        "\n",
        "# Specify the destination directory (same as the zip file directory)\n",
        "destination_directory = os.path.dirname(zip_file_path)\n",
        "\n",
        "# Create the destination directory if it doesn't exist\n",
        "os.makedirs(destination_directory, exist_ok=True)\n",
        "\n",
        "# Unzip the file\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(destination_directory)\n",
        "\n",
        "print(f\"File unzipped to: {destination_directory}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e6uaLJZgcLu4",
        "outputId": "fae07211-797a-4833-fbf1-09e8d32984f3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File unzipped to: /content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "train = pd.read_csv('/content/train.csv')\n",
        "train_labels = pd.read_csv('/content/train_labels.csv')\n",
        "target_pairs = pd.read_csv('/content/target_pairs.csv')\n",
        "\n",
        "print('training data:', train.shape)\n",
        "print('train label:', train_labels.shape)\n",
        "print('target_pairs:', target_pairs.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jBqM1A8ScQR1",
        "outputId": "e81195a1-6106-43d9-97d2-acc4db1ccc69"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training data: (1917, 558)\n",
            "train label: (1917, 425)\n",
            "target_pairs: (424, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================================================================\n",
        "# 1. セットアップとデータ読み込み\n",
        "# ===================================================================\n",
        "import os\n",
        "import gc\n",
        "import pickle\n",
        "import warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Input\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# --- データパス ---\n",
        "TRAIN_PATH = '/content/train.csv'\n",
        "LABELS_PATH = '/content/train_labels.csv'\n",
        "TARGET_PAIRS_PATH = '/content/target_pairs.csv'\n",
        "OOF_PREDS_PATH = '/content/drive/MyDrive/kaggle notebook/MITSUI&CO. Commodity Prediction Challenge/oof_predictions_lgbm.csv' # ベースライン予測\n",
        "\n",
        "# 保存用ディレクトリ\n",
        "MODEL_DIR = '/content/drive/MyDrive/kaggle notebook/MITSUI&CO. Commodity Prediction Challenge/lstm_models_masked_ver1'\n",
        "SCALER_DIR = '/content/drive/MyDrive/kaggle notebook/MITSUI&CO. Commodity Prediction Challenge/scalers_masked_ver1'\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "os.makedirs(SCALER_DIR, exist_ok=True)\n",
        "\n",
        "# --- データを読み込み、結合する ---\n",
        "print(\"データの読み込みを開始します...\")\n",
        "train_df = pd.read_csv(TRAIN_PATH)\n",
        "labels_df = pd.read_csv(LABELS_PATH)\n",
        "target_pairs_df = pd.read_csv(TARGET_PAIRS_PATH)\n",
        "featured_df = pd.merge(train_df, labels_df, on='date_id', how='left')\n",
        "print(\"データの読み込みと結合が完了しました。\")\n",
        "\n",
        "# ===================================================================\n",
        "# 2. 特徴量エンジニアリング（強化版）\n",
        "# ===================================================================\n",
        "print(\"\\n特徴量エンジニアリングを開始します...\")\n",
        "\n",
        "# --- 基本的なテクニカル指標 ---\n",
        "# (ユーザー様の `create_technical_features` と `create_cross_asset_features` を使用)\n",
        "def create_technical_features(df, price_cols, window_sizes=[5, 10, 20]):\n",
        "    \"\"\"テクニカル指標を生成する\"\"\"\n",
        "    features = df.copy()\n",
        "    for col in price_cols:\n",
        "        if col in df.columns:\n",
        "            for window in window_sizes:\n",
        "                features[f'{col}_MA_{window}'] = features[col].rolling(window=window, min_periods=1).mean()\n",
        "                features[f'{col}_STD_{window}'] = features[col].rolling(window=window, min_periods=1).std()\n",
        "            features[f'{col}_Return_1d'] = features[col].pct_change(1)\n",
        "            features[f'{col}_LogReturn_1d'] = np.log(features[col] / features[col].shift(1))\n",
        "            ma_20 = features[col].rolling(window=20, min_periods=1).mean()\n",
        "            std_20 = features[col].rolling(window=20, min_periods=1).std()\n",
        "            features[f'{col}_BB_Upper'] = ma_20 + (2 * std_20)\n",
        "            features[f'{col}_BB_Lower'] = ma_20 - (2 * std_20)\n",
        "    return features\n",
        "\n",
        "def create_cross_asset_features(df, asset_groups):\n",
        "    \"\"\"クロスアセット特徴量を生成する\"\"\"\n",
        "    features = df.copy()\n",
        "    for group_name, assets in asset_groups.items():\n",
        "        available_assets = [asset for asset in assets if asset in df.columns]\n",
        "        if len(available_assets) >= 2:\n",
        "            group_returns = df[available_assets].pct_change().rolling(5).mean()\n",
        "            features[f'{group_name}_Momentum_5d'] = group_returns.mean(axis=1)\n",
        "            features[f'{group_name}_Volatility_5d'] = df[available_assets].pct_change().rolling(5).std().mean(axis=1)\n",
        "    return features\n",
        "\n",
        "\n",
        "PRICE_COLS = [col for col in train_df.columns if '_Close' in col or '_adj_close' in col or 'FX_' in col]\n",
        "featured_df = create_technical_features(featured_df, PRICE_COLS)\n",
        "asset_groups = {\n",
        "    'Metals': [col for col in PRICE_COLS if 'LME' in col],\n",
        "    'Precious_Metals': [col for col in PRICE_COLS if 'Gold' in col or 'Silver' in col or 'Platinum' in col],\n",
        "    'FX': [col for col in PRICE_COLS if 'FX' in col]\n",
        "}\n",
        "featured_df = create_cross_asset_features(featured_df, asset_groups)\n",
        "\n",
        "# --- スプレッド/レシオ特徴量 ---\n",
        "for _, row in target_pairs_df.iterrows():\n",
        "    pair = row['pair']\n",
        "    if ' - ' in pair:\n",
        "        asset1, asset2 = pair.split(' - ')\n",
        "        if asset1 in featured_df.columns and asset2 in featured_df.columns:\n",
        "            featured_df[f'SPREAD_{asset1}_{asset2}'] = featured_df[asset1] - featured_df[asset2]\n",
        "            featured_df[f'RATIO_{asset1}_{asset2}'] = featured_df[asset1] / (featured_df[asset2] + 1e-6)\n",
        "\n",
        "# --- ラグ/移動平均乖離 特徴量 ---\n",
        "lags = [1, 5, 10]\n",
        "windows = [5, 10, 20]\n",
        "for col in PRICE_COLS:\n",
        "    for lag in lags:\n",
        "        featured_df[f'{col}_lag_{lag}'] = featured_df[col].shift(lag)\n",
        "    for window in windows:\n",
        "        ma = featured_df[col].rolling(window=window).mean()\n",
        "        featured_df[f'{col}_ma_gap_{window}'] = featured_df[col] / ma\n",
        "\n",
        "# --- OOF予測（メタ特徴量）の読み込みと結合 ---\n",
        "try:\n",
        "    oof_df = pd.read_csv(OOF_PREDS_PATH, index_col=0)\n",
        "    oof_df.columns = [f'target_{i}' for i in range(oof_df.shape[1])]\n",
        "    oof_meta_features = oof_df.add_suffix('_oof_pred')\n",
        "    featured_df_with_meta = featured_df.reset_index(drop=True).merge(\n",
        "        oof_meta_features.reset_index(drop=True), left_index=True, right_index=True, how='left'\n",
        "    )\n",
        "    print(\"OOF予測をメタ特徴量として追加しました。\")\n",
        "except FileNotFoundError:\n",
        "    print(\"警告: OOF予測ファイルが見つかりません。メタ特徴量なしで続行します。\")\n",
        "    featured_df_with_meta = featured_df.copy()\n",
        "\n",
        "# --- 最終的な前処理 ---\n",
        "# 特徴量の欠損値を埋める\n",
        "feature_cols_only = [col for col in featured_df_with_meta.columns if not col.startswith('target_')]\n",
        "featured_df_with_meta[feature_cols_only] = featured_df_with_meta[feature_cols_only].ffill().bfill()\n",
        "featured_df_with_meta.replace([np.inf, -np.inf], 0, inplace=True) # 0で埋める方が安全な場合もある\n",
        "\n",
        "print(f\"特徴量生成完了。最終的なデータ形状: {featured_df_with_meta.shape}\")\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2P_HnxfNcQ0J",
        "outputId": "b85105bf-8328-4210-8962-7d676d7fadf3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "データの読み込みを開始します...\n",
            "データの読み込みと結合が完了しました。\n",
            "\n",
            "特徴量エンジニアリングを開始します...\n",
            "OOF予測をメタ特徴量として追加しました。\n",
            "特徴量生成完了。最終的なデータ形状: (1917, 4440)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================================================================\n",
        "# 3. ヘルパー関数とモデル定義\n",
        "# ===================================================================\n",
        "\n",
        "def create_dataset_with_masking(df, target_col, oof_col, feature_cols, sequence_length, batch_size):\n",
        "    \"\"\"ターゲットのNaNを削除せず、そのままシーケンスを生成する\"\"\"\n",
        "    data = df.copy()\n",
        "\n",
        "    current_features = feature_cols.copy()\n",
        "    if oof_col in data.columns:\n",
        "        current_features.append(oof_col)\n",
        "\n",
        "    X_data = data[current_features].ffill().bfill()\n",
        "    y_data = data[target_col] # NaNを含むターゲット\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X_data)\n",
        "\n",
        "    dataset = tf.keras.utils.timeseries_dataset_from_array(\n",
        "        X_scaled, y_data,\n",
        "        sequence_length=sequence_length,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False\n",
        "    )\n",
        "    return dataset, scaler\n",
        "\n",
        "def masked_mse(y_true, y_pred):\n",
        "    \"\"\"y_trueがNaNである部分を無視する平均二乗誤差（MSE）\"\"\"\n",
        "    mask = tf.math.is_finite(y_true)\n",
        "    y_true_masked = tf.boolean_mask(y_true, mask)\n",
        "    y_pred_masked = tf.boolean_mask(y_pred, mask)\n",
        "    # MeanSquaredErrorクラスのインスタンスを生成して損失を計算する\n",
        "    loss_fn = tf.keras.losses.MeanSquaredError()\n",
        "    return loss_fn(y_true_masked, y_pred_masked)\n",
        "\n",
        "def build_lstm_model(input_shape):\n",
        "    \"\"\"LSTMモデルを構築・コンパイルする\"\"\"\n",
        "    model = Sequential([\n",
        "        Input(shape=input_shape),\n",
        "        LSTM(units=128, return_sequences=True),\n",
        "        Dropout(0.3),\n",
        "        LSTM(units=64, return_sequences=False),\n",
        "        Dropout(0.3),\n",
        "        Dense(units=64, activation='relu'),\n",
        "        Dropout(0.3),\n",
        "        Dense(units=1, activation='linear')\n",
        "    ])\n",
        "    # ★★★ カスタム損失関数を指定 ★★★\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.01), loss=masked_mse)\n",
        "    return model"
      ],
      "metadata": {
        "id": "ke24h_hKcURM"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================================================================\n",
        "# 4. 全ターゲットの学習と保存\n",
        "# ===================================================================\n",
        "\n",
        "# --- パラメータ設定 ---\n",
        "SEQUENCE_LENGTH = 30\n",
        "BATCH_SIZE = 64 # データ量が増えたのでバッチサイズを少し戻しても良い\n",
        "EPOCHS = 100\n",
        "PATIENCE = 15\n",
        "\n",
        "# --- ターゲットと特徴量のリスト準備 ---\n",
        "target_cols = [col for col in labels_df.columns if col.startswith('target_')]\n",
        "base_feature_cols = [col for col in featured_df_with_meta.columns if not col.startswith('target_') and col != 'date_id']\n",
        "\n",
        "# --- 結果記録用のリスト ---\n",
        "training_results = []\n",
        "\n",
        "print(f\"\\n--- 全{len(target_cols)}ターゲットの学習を開始します ---\")\n",
        "\n",
        "for i, target_col in enumerate(target_cols):\n",
        "    print(f\"\\n[{i+1}/{len(target_cols)}] ターゲット '{target_col}' の処理を開始...\")\n",
        "    gc.collect()\n",
        "\n",
        "    try:\n",
        "        oof_col = f'{target_col}_oof_pred'\n",
        "\n",
        "        # 1. データ準備\n",
        "        dataset, scaler = create_dataset_with_masking(\n",
        "            featured_df_with_meta, target_col, oof_col,\n",
        "            base_feature_cols, SEQUENCE_LENGTH, BATCH_SIZE\n",
        "        )\n",
        "\n",
        "        # 2. データセット分割\n",
        "        DATASET_SIZE = tf.data.experimental.cardinality(dataset).numpy()\n",
        "        if DATASET_SIZE < 2:\n",
        "            print(\"  > データセットが小さすぎるためスキップします。\")\n",
        "            training_results.append({'target': target_col, 'val_rmse': np.nan, 'status': 'Skipped (Too small)'})\n",
        "            continue\n",
        "\n",
        "        train_size = int(0.85 * DATASET_SIZE)\n",
        "        train_dataset = dataset.take(train_size).cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "        val_dataset = dataset.skip(train_size).cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "        # 3. モデルの構築と学習\n",
        "        input_shape = train_dataset.element_spec[0].shape[1:]\n",
        "        lstm_model = build_lstm_model(input_shape)\n",
        "        early_stopping = EarlyStopping(monitor='val_loss', patience=PATIENCE, restore_best_weights=True)\n",
        "\n",
        "        print(f\"  > 学習を開始... (Train: {train_size} batches, Val: {DATASET_SIZE - train_size} batches)\")\n",
        "        history = lstm_model.fit(\n",
        "            train_dataset, validation_data=val_dataset,\n",
        "            epochs=EPOCHS, callbacks=[early_stopping], verbose=0\n",
        "        )\n",
        "\n",
        "        # 4. 評価 (マスクを考慮)\n",
        "        val_preds = lstm_model.predict(val_dataset).flatten()\n",
        "        y_val = np.concatenate([y.numpy() for _, y in val_dataset])\n",
        "\n",
        "        mask = ~np.isnan(y_val) # NaNでない部分のマスク\n",
        "        if np.sum(mask) == 0:\n",
        "            val_rmse = np.nan # 検証セットにラベルが一つもなかった\n",
        "        else:\n",
        "            val_rmse = np.sqrt(mean_squared_error(y_val[mask], val_preds[mask]))\n",
        "\n",
        "        best_epoch = np.argmin(history.history['val_loss']) + 1\n",
        "        print(f\"  > 学習完了。検証RMSE: {val_rmse:.4f} (at epoch {best_epoch})\")\n",
        "\n",
        "        # 5. モデルとスケーラーの保存\n",
        "        lstm_model.save(os.path.join(MODEL_DIR, f'model_{target_col}.keras'))\n",
        "        with open(os.path.join(SCALER_DIR, f'scaler_{target_col}.pkl'), 'wb') as f:\n",
        "            pickle.dump(scaler, f)\n",
        "        print(f\"  > モデルとスケーラーを保存しました。\")\n",
        "\n",
        "        training_results.append({'target': target_col, 'val_rmse': val_rmse, 'status': 'Success'})\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"!!! エラー発生: {target_col} の処理中にエラー: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        training_results.append({'target': target_col, 'val_rmse': np.nan, 'status': f'Failed: {e}'})\n",
        "\n",
        "# --- サマリー表示 ---\n",
        "print(\"\\n--- 全てのターゲットの処理が完了しました ---\")\n",
        "results_df = pd.DataFrame(training_results)\n",
        "if not results_df.empty:\n",
        "    print(\"\\n学習結果サマリー:\")\n",
        "    print(results_df.head())\n",
        "    print(f\"\\n成功: {results_df[results_df['status']=='Success'].shape[0]} 件\")\n",
        "    print(f\"失敗/スキップ: {results_df[results_df['status']!='Success'].shape[0]} 件\")\n",
        "    avg_rmse = results_df['val_rmse'].mean()\n",
        "    print(f\"成功したモデルの平均検証RMSE: {avg_rmse:.4f}\")\n",
        "    results_df.to_csv('/content/drive/MyDrive/kaggle notebook/MITSUI&CO. Commodity Prediction Challenge/lstm_masked_training_results.csv', index=False)\n",
        "    print(\"\\n学習結果のサマリーをCSVに保存しました。\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 642
        },
        "id": "Qmrow0FVcUI5",
        "outputId": "2492ef88-03bf-448d-ad3a-bbf828755605"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- 全424ターゲットの学習を開始します ---\n",
            "\n",
            "[1/424] ターゲット 'target_0' の処理を開始...\n",
            "  > 学習を開始... (Train: 25 batches, Val: 5 batches)\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  > 学習完了。検証RMSE: 0.0106 (at epoch 41)\n",
            "  > モデルとスケーラーを保存しました。\n",
            "\n",
            "[2/424] ターゲット 'target_1' の処理を開始...\n",
            "  > 学習を開始... (Train: 25 batches, Val: 5 batches)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-193109004.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"  > 学習を開始... (Train: {train_size} batches, Val: {DATASET_SIZE - train_size} batches)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         history = lstm_model.fit(\n\u001b[0m\u001b[1;32m     51\u001b[0m             \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mearly_stopping\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    375\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDistributedIterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m             ):\n\u001b[0;32m--> 220\u001b[0;31m                 \u001b[0mopt_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmulti_step_on_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mopt_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    841\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mwithout_tracing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 843\u001b[0;31m         _frequent_tracing_detector_manager.called_without_tracing(\n\u001b[0m\u001b[1;32m    844\u001b[0m             self._key_for_call_stats)\n\u001b[1;32m    845\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36mcalled_without_tracing\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    194\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcalled_without_tracing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m       \u001b[0mdetector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_detector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m       \u001b[0mdetector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalled_without_tracing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_get_detector\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mthreading\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m   \u001b[0;32mdef\u001b[0m \u001b[0m_get_detector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_detectors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_detectors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_FrequentTracingDetector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}